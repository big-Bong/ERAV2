{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 6 - Part B\n",
        "\n",
        "#### 1. Less than 20K parameters\n",
        "#### 2. Less than 20 epochs\n",
        "#### 3. At least 99.4% accuracy\n",
        "#### 4. Have used batch normalization and dropout"
      ],
      "metadata": {
        "id": "aUf77khEnxi9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "source": [
        "#Import all necessary libraries\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network architecture\n",
        "\n",
        "\n",
        "\n",
        "1.   1st Convolution layer with 1 input and 16 output channels\n",
        "2.   This is followed by a batch normalization and a max pool layer\n",
        "3. A dropout layer is added after this\n",
        "4. Another block of exact same layers (convolution, batch norm, max pool and dropout). This has 16 channels as input and 32 channels as output\n",
        "5. Third convolution layer with 32 input channels and 32 output channels. This helps in increasing our receptive field while using small number of parameters\n",
        "6. Final convolution layer/output layer, which has 32 inputs and 10 output channels (corresponding to our 10 classes)\n",
        "7. We use a ReLU activation after each of our convolulation layer\n",
        "8. We unroll the final output into a 1X10 output array and apply log_softmax to get out final output\n",
        "\n"
      ],
      "metadata": {
        "id": "0MDQpaf5obIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 16, 3)\n",
        "    self.norm1 = nn.BatchNorm2d(16)\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)\n",
        "    self.drop1 = nn.Dropout(0.10)\n",
        "    self.conv2 = nn.Conv2d(16, 32, 3)\n",
        "    self.norm2 = nn.BatchNorm2d(32)\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "    self.drop2 = nn.Dropout(0.10)\n",
        "    self.conv3 = nn.Conv2d(32, 32, 3)\n",
        "    self.conv4 = nn.Conv2d(32, 10, 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # After 1st conv -> n_in = 28, p = 0, s = 1, k = 3, n_out = 26, j_in = 1, j_out = 1, r_in = 1, r_out = 3\n",
        "    # After 1st Max Pool -> n_in = 26, p = 0, s = 2, k = 2, n_out = 13, j_in = 1, j_out = 2, r_in = 3, r_out = 4\n",
        "    x = self.drop1(self.pool1(self.norm1(F.relu(self.conv1(x)))))\n",
        "    # After 2nd conv -> n_in = 13, p = 0, s = 1, k = 3, n_out = 11, j_in = 2, j_out = 2, r_in = 4, r_out = 8\n",
        "    # After 2nd Max Pool -> n_in = 11, p = 0, s = 2, k = 2, n_out = 5, j_in = 2, j_out = 4, r_in = 8, r_out = 10\n",
        "    x = self.drop2(self.pool2(self.norm2(F.relu(self.conv2(x)))))\n",
        "    # After 3rd conv -> n_in = 5, p = 0, s = 1, k = 3, n_out = 3, j_in = 4, j_out = 4, r_in = 10, r_out = 18\n",
        "    # After 4th conv -> n_in = 13, p = 0, s = 1, k = 3, n_out = 11, j_in = 2, j_out = 2, r_in = 4, r_out = 26\n",
        "    x = F.relu(self.conv4(F.relu(self.conv3(x))))\n",
        "    x = x.view(-1,10)\n",
        "    return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fwwhMKv4a9p9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f4139bd-a289-4adb-f0a0-00ab0e7099b2"
      },
      "source": [
        "#Creating model summary\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available() #Check if CUDA is available or not\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\") #Use CUDA if available\n",
        "model = Net().to(device) #Load model to device\n",
        "summary(model, input_size=(1, 28, 28)) #Create summary if input image is 28X28"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 26, 26]             160\n",
            "       BatchNorm2d-2           [-1, 16, 26, 26]              32\n",
            "         MaxPool2d-3           [-1, 16, 13, 13]               0\n",
            "           Dropout-4           [-1, 16, 13, 13]               0\n",
            "            Conv2d-5           [-1, 32, 11, 11]           4,640\n",
            "       BatchNorm2d-6           [-1, 32, 11, 11]              64\n",
            "         MaxPool2d-7             [-1, 32, 5, 5]               0\n",
            "           Dropout-8             [-1, 32, 5, 5]               0\n",
            "            Conv2d-9             [-1, 32, 3, 3]           9,248\n",
            "           Conv2d-10             [-1, 10, 1, 1]           2,890\n",
            "================================================================\n",
            "Total params: 17,034\n",
            "Trainable params: 17,034\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.28\n",
            "Params size (MB): 0.06\n",
            "Estimated Total Size (MB): 0.35\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataloader object for training and testing model"
      ],
      "metadata": {
        "id": "k0XDa4CxsKMC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9977ca0-40c6-42bb-819c-0727f5227e3e"
      },
      "source": [
        "torch.manual_seed(1) #Set seed for random number generator. Makes sure to generate the same numbers given the same input. Helps to keep code reproducible.\n",
        "batch_size = 128 #Set batch size. Number of images which will be processed in one go.\n",
        "\n",
        "kwargs = {'num_workers': 2, 'pin_memory': True} if use_cuda else {} #Set number of threads and optimize file copy operations\n",
        "train_loader = torch.utils.data.DataLoader( #Create data loader objects and apply different compositions. Normalize all images.\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.9, 1.1)),\n",
        "                        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader( #Dataloader object for test dataset. Normalize all images.\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 73139630.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 87715926.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 24516960.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 16933803.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating training and testing methods"
      ],
      "metadata": {
        "id": "Cdg0f5dqsWD8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train() #Set model to train mode\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad() #Preventing gradient accumulation\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target) #Negative log likelihood loss\n",
        "        loss.backward() #Backpropagation. Weight calculation.\n",
        "        optimizer.step() #Update parameter values\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval() #Set model to test\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and test the model"
      ],
      "metadata": {
        "id": "jVwL-dxhsl9L"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f000955-d319-4f3b-db56-a3700732bc86"
      },
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #Using SGD optimizer\n",
        "\n",
        "for epoch in range(1, 19): #Run for 18 epochs\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.33171555399894714 batch_id=468: 100%|██████████| 469/469 [00:55<00:00,  8.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0588, Accuracy: 9807/10000 (98.07%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.14228419959545135 batch_id=468: 100%|██████████| 469/469 [00:54<00:00,  8.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0472, Accuracy: 9852/10000 (98.52%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.1556026041507721 batch_id=468: 100%|██████████| 469/469 [00:54<00:00,  8.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0372, Accuracy: 9878/10000 (98.78%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.07359858602285385 batch_id=468: 100%|██████████| 469/469 [00:54<00:00,  8.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0344, Accuracy: 9883/10000 (98.83%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.04077104106545448 batch_id=468: 100%|██████████| 469/469 [00:55<00:00,  8.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0340, Accuracy: 9885/10000 (98.85%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.015617698431015015 batch_id=468: 100%|██████████| 469/469 [00:54<00:00,  8.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0277, Accuracy: 9906/10000 (99.06%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.17869256436824799 batch_id=468: 100%|██████████| 469/469 [00:54<00:00,  8.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0299, Accuracy: 9898/10000 (98.98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.09060642123222351 batch_id=468: 100%|██████████| 469/469 [00:55<00:00,  8.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0249, Accuracy: 9924/10000 (99.24%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.06253983825445175 batch_id=468: 100%|██████████| 469/469 [00:54<00:00,  8.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0275, Accuracy: 9912/10000 (99.12%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.0598558634519577 batch_id=468: 100%|██████████| 469/469 [00:55<00:00,  8.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0246, Accuracy: 9917/10000 (99.17%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.051030904054641724 batch_id=468: 100%|██████████| 469/469 [00:54<00:00,  8.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0212, Accuracy: 9929/10000 (99.29%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.030841229483485222 batch_id=468: 100%|██████████| 469/469 [00:54<00:00,  8.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0234, Accuracy: 9924/10000 (99.24%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.029306868091225624 batch_id=468: 100%|██████████| 469/469 [00:55<00:00,  8.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0233, Accuracy: 9929/10000 (99.29%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.061530470848083496 batch_id=468: 100%|██████████| 469/469 [00:54<00:00,  8.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0209, Accuracy: 9936/10000 (99.36%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.09780261665582657 batch_id=468: 100%|██████████| 469/469 [00:54<00:00,  8.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0216, Accuracy: 9940/10000 (99.40%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.06268691271543503 batch_id=468: 100%|██████████| 469/469 [00:55<00:00,  8.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0220, Accuracy: 9925/10000 (99.25%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.1130494698882103 batch_id=468: 100%|██████████| 469/469 [00:54<00:00,  8.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0220, Accuracy: 9927/10000 (99.27%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.09432690590620041 batch_id=468: 100%|██████████| 469/469 [00:55<00:00,  8.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0217, Accuracy: 9935/10000 (99.35%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}